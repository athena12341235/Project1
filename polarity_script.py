# -*- coding: utf-8 -*-
"""polarity_script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-icwhvZg4gfsZOXiFs6nyr1BvCAEkH4q
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the dataset
df = pd.read_csv('/content/review_polarity_clean.csv')

X = df['clean_text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

# --- Pipeline: TF-IDF + Linear SVM ---
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('svm', LinearSVC())
])

# --- Define parameter grid for GridSearchCV ---
param_grid = {
    'tfidf__max_features': [5000, 10000, 20000],
    'tfidf__ngram_range': [(1,1), (1,2)],   # unigrams vs unigrams+bigrams
    'svm__C': [0.01, 0.1, 1, 10],           # regularization strength
}

# --- Grid search with 5-fold cross-validation ---
grid = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2
)

grid.fit(X_train, y_train)   # X_train_raw = raw text column, not vectorized

print("Best parameters:", grid.best_params_)
print("Best CV accuracy:", grid.best_score_)

# --- Evaluate on held-out test set ---
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)  # pass raw text

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# --- Confusion Matrix ---
disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

import pandas as pd

# --- Save GridSearchCV results to a log file ---
results_df = pd.DataFrame(grid.cv_results_)

# Keep only useful columns
log_df = results_df[
    ['params',
     'mean_test_accuracy', 'std_test_accuracy',
     'mean_test_f1_macro', 'std_test_f1_macro',
     'mean_test_f1_weighted', 'std_test_f1_weighted',
     'rank_test_f1_macro']
]

# --- Save GridSearchCV results + final test performance to a text log ---
log_file = "svm_gridsearch_log.txt"

with open(log_file, "w") as f:
    f.write("=== GridSearchCV Results (Top 10 by Macro-F1) ===\n\n")

    # Sort results by macro-F1 and print top 10
    sorted_idx = results_df.sort_values(by="mean_test_f1_macro", ascending=False).head(10)
    for i, row in sorted_idx.iterrows():
        f.write(f"Params: {row['params']}\n")
        f.write(f"  Accuracy: {row['mean_test_accuracy']:.4f} ± {row['std_test_accuracy']:.4f}\n")
        f.write(f"  F1-macro: {row['mean_test_f1_macro']:.4f} ± {row['std_test_f1_macro']:.4f}\n")
        f.write(f"  F1-weighted: {row['mean_test_f1_weighted']:.4f} ± {row['std_test_f1_weighted']:.4f}\n\n")

    f.write("\n=== Best Parameters ===\n")
    f.write(str(grid.best_params_) + "\n\n")

    f.write("=== Final Test Performance ===\n")
    f.write(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
    f.write(classification_report(y_test, y_pred, digits=4))

print(f"Log written to {log_file}")